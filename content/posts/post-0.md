+++
title = "Running Deepseek R1 Locally: A Step-by-Step Guide"
date = 2025-01-30
+++

# Running Deepseek R1 Locally: A Step-by-Step Guide to Privacy and Efficiency

In today’s digital age, privacy and control over your data are more important than ever. If you’ve been curious about running Deepseek locally on your machine, you’re in the right place! This guide will walk you through a simple process to set up Deepseek using Ollama, ensuring that your queries remain private and under your complete control.

---

## Why Run Deepseek Locally?

Before diving into the instructions, let’s explore why running Deepseek locally is a great idea:

1. **Privacy**: When you run Deepseek on your local machine, no one else can access or collect your data. You retain full ownership of your information.
2. **Speed and Efficiency**: Running models locally can be faster than relying on cloud-based services, especially for repetitive tasks or large-scale projects.
3. **Customization**: Local setups allow you to tweak configurations and integrate with other tools in ways that aren’t always possible with hosted solutions.

Now that we’ve covered the benefits, let’s get started!

---

## Step-by-Step Guide: Installing Ollama and Running Deepseek

### 1. Download Ollama
Ollama is a lightweight, open-source tool designed for running large language models locally. To install it:

- Visit the official [Ollama website](https://ollama.ai/).
- Click on the **Download** button.
- Select your operating system (Windows, macOS, or Linux) from the dropdown menu.

### 2. Install Ollama
Once you’ve downloaded the installer:
- Double-click the installer file.
- Follow the on-screen instructions to complete the installation process.

### 3. Verify Installation
Open your terminal of choice (cmd, PowerShell, Git Bash, etc.) and type:

```bash
ollama
```

This command will display a list of available commands and flags. If you see this output, congratulations—you’ve successfully installed Ollama!

---

## Downloading the Deepseek Model

Now that Ollama is installed, it’s time to download the Deepseek model.

### 1. Navigate to the Deepseek Page
- Go back to the [Ollama website](https://ollama.ai/).
- Search for **"deepseek-r1"** in the search bar.
- Select the size of the model you want to run (up to 70b parameters for RTX 4090).

### 2. Copy the Command
Once you’ve selected your desired model size, copy the command provided on the right side of the page.

### 3. Run the Command
Paste the copied command into your terminal and press Enter. This will begin downloading and running the Deepseek model locally on your machine.

---

## Start Using Deepseek!

Once the installation is complete, you’re ready to start using Deepseek. Simply type your prompts in the terminal, and enjoy the benefits of a fast, private, and efficient AI experience.

---

## Conclusion

Running Deepseek locally with Ollama is a fantastic way to maintain privacy and control over your data. By following this guide, you’ve taken a significant step toward empowering yourself in the digital world. If you found this helpful, consider sharing it with others who might benefit from running AI models privately and efficiently.

Happy exploring!

---

Note: this post is created by 32b Deepseek R1 running locally on my machine.